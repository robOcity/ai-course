# Week 8 - Machine Learning with Neural Networks

1. Motivation: Continuing examination of machine learning in artificially intelligent systems.  This week we focus on neural networks.

1. Learning objectives
    1. Select models that do not perform well, remove them from consideration and those that are left are good-enough or _probably approximately correct_.

    1. Summarize the relationship between the number of features in a model and the density of data points.

    1. Explain how neural network functions including the neurons, activation function, weights, forward propagation and back propagation.

1. Readings
    1. Read Chapter 18 - Learning from Examples (18.5, 18.6, 18.7, 18.10 and 18.11)

    1. Text: Stuart Russel and Peter Norvig. _Artificial Intelligence: A Modern Approach_ (3rd ed). Prentice-Hall. 2010.

1. From the experts
    1. Watch: [Boosting Lecture](https://youtu.be/UHBmv7qCey4)

    1. Watch: [Boosting Tutorial](https://youtu.be/gmok1h8wG-Q)

    1. Read: [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d) - An introduction to ensemble methods that compares the bagging and boosting approaches.

1. Discussion
    1. Read Section 26.3 - The Ethics and Risks of Developing Artificial Intelligence in our textbook.  Analyze the potential threats from AI technology to society.  What threats are the most serious, and how might they be combated?  How do they compare to the potential benefits?  _Support your position and post your discussion by midnight Wednesday.  Reply to three other students by midnight Sunday._

    1. First, watch [Google I/O 2018 Google's Duplex Demo Stole the Show](https://youtu.be/NO0-5MuJvew) on YouTube. Then discuss the whether, or not, you have ethical concerns about Google Duplex?  If so, what are they, and why?  If not, why not?  Please respond in a short essay (of no more than 300 words) supporting your position and cite supporting materials.  _Support your position and post your discussion by midnight Wednesday.  Reply to two other students by midnight Sunday._

1. Citations
    1. Patrick Winston. 6.034 Artificial Intelligence. Fall 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, [https://ocw.mit.edu](https://ocw.mit.edu). License: Creative Commons BY-NC-SA.

    1. Noss, J. (2016, December 02). 6.034 Recitation 10: Boosting (Adaboost). Retrieved August 19, 2018, from https://youtu.be/gmok1h8wG-Q.  Recitation for 6.034 Artificial Intelligence at MIT, Fall 2016 Covers Adaboost, using 2012 Quiz 4 as an example problem. In particular: - Initializing and updating weights - Calculating error rates for weak classifiers - Selecting the "best" weak classifier - Computing voting power - Deciding when to stop (three exit conditions) - General properties of Adaboost.

    1. Grover, P. (2017, December 09). Gradient Boosting from scratch – ML Review – Medium. Retrieved August 19, 2018, from https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d - - A concise introduction to ensemble learning that compares bagging to boosting approaches. 

