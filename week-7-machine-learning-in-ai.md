# Week 7 - Machine Learning in AI

1. Motivation: Imagine trying to program a agent to recognize hand-written numbers, or to predict home prices.  Instead of programming it directly, let's use apply algorithms that learn from the experience.  In this chapter, we will consider machine learning from the perspective of artificial intelligence.  This week we focus on decision trees and ways to improve and evaluate model performance.

1. Learning objectives

    1. Explain how computation learning theory eliminates poorly performing models leaving those that are probably approximately correct.

    1. Construct a decision tree by recursively selecting the most important feature.

    1. Compare common approaches to improving the performance of decision trees.

    1. Evaluate model performance and how it can be improved through cross-validation and regularization.

    1. Understand the trade-off between bias, variance and model complexity.

1. Readings
    1. Read Chapter 18 - Learning from Examples (sections 18.1 through 18.4)

    1. Text: Stuart Russel and Peter Norvig. _Artificial Intelligence: A Modern Approach_ (3rd ed). Prentice-Hall. 2010.

1. From the experts

    1. Watch -- [Lecture 10: Introduction to Learning, Nearest Neighbors](https://youtu.be/09mb78oiPkA)

    1. Watch -- [Lecture 11: Learning: Identification Trees, Disorder](https://youtu.be/SXBG3RGr_Rc)

    1. Cite: Patrick Winston. 6.034 Artificial Intelligence. Fall 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, [https://ocw.mit.edu](https://ocw.mit.edu). License: Creative Commons BY-NC-SA.

1. Discussion

    1. Consider the problem faced by an infant learning to speak and understand a language.  Explain how this process fits into the general learning model. Describe the precepts and actions of the infant, and the types of learning the infant must do. Describe the subfunctions the infant is trying to learn in terms of inputs and outputs, and available example data.  _Please submit your post by Wednesday evening at midnight and comment on two other students work by Sunday evening at midnight._

    1. Suppose we generate a training set from a decision tree and then apply decision-tree learning to that training set. Is it the case that the learning algorithm will eventually return the correct tree as the training-set size goes to infinity? Why or why not? _Please submit your post by Wednesday evening at midnight and comment on two other students work by Sunday evening at midnight._
