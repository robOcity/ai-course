{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MSDS688 -- Artifical Intelligence\n",
    "\n",
    "## Week 7 - Machine Learning in AI\n",
    "\n",
    "![Wolfgang von Kempelen designed a speaking machine](../images/kempelen-speaking-machine.png)\n",
    "\n",
    "About a decade later, a Hungarian engineer named Wolfgang von Kempelen designed a speaking machine using an ivory glottis, bellows for lungs, a leather vocal tract with a hinged tongue, a rubber oral cavity and mouth, and a nose with two little pipes as nostrils. Its pronouncements were more whimsical than those of Mical’s talking heads: “my wife is my friend”, for example, and “come with me to Paris”.\n",
    "\n",
    "Cite: Riskin, J. (n.d.). Frolicsome Engines: The Long Prehistory of Artificial Intelligence. Retrieved April 10, 2018, from [https://publicdomainreview.org/2016/05/04/frolicsome-engines-the-long-prehistory-of-artificial-intelligence/](https://publicdomainreview.org/2016/05/04/frolicsome-engines-the-long-prehistory-of-artificial-intelligence/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review - Concepts and techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quiz / Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "_Note: Start with a promise_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Explain how computation learning theory eliminates poorly performing models leaving those that are probably approximately correct.\n",
    "\n",
    "1. Construct a decision tree by recursively selecting the most important feature.\n",
    "\n",
    "1. Compare common approaches to improving the performance of decision trees.\n",
    "\n",
    "1. Evaluate model performance and how it can be improved through cross-validation and regularization.\n",
    "\n",
    "1. Understand the trade-off between bias, variance and model complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Learning\n",
    "\n",
    "**Goal: To teach the agent a function that maps from an input image to one of those strings**\n",
    "\n",
    "1. Unsupervised learning\n",
    "\n",
    "1. Re-enforcement learning\n",
    "\n",
    "1. Supervised learning -- Our focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "Unsupervised Learning the agent learns patterns in the input even though no explicit feedback is supplied. The most common type is clustering: detecting potential useful clusters of input examples.\n",
    "\n",
    "_Example: A taxi agent would develop a concept of good traffic days and bad traffic days without ever being given labeled examples._\n",
    "\n",
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Re-inforcement learning\n",
    "\n",
    "In Reinforcement Learning the agent learns from a series of reinforcements—rewards or punishments.\n",
    "\n",
    "_Example: Take a game playing agent and reward it for good play and penalize it for bad.  Eventually, the agent will learn what actions are best for a particular circumstance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised learning\n",
    "\n",
    "In Supervised Learning the agent observes some example input-output pairs and learns a function that maps from input to output.\n",
    "\n",
    "_Example: Providing a agent an image of a cat or a dog. To teach this agent, we will give a lot of input-output pairs like {cat image-\"cat\"}, {dog image-\"dog\"} to the agent._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Cite** \"Learning.ipynb.\" Aima-Python, GitHub, May 2018, [github.com/aimacode/aima-python](github.com/aimacode/aima-python). Python implementation of algorithms from Russell And Norvig's \"Artificial Intelligence - A Modern Approach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Bias and Variance\n",
    "\n",
    "* Bias\n",
    "    + Informal: A systematic and consistent error in a model's results.\n",
    "    + Formal: The amount the expected value of the results differ from the true value.\n",
    "\n",
    "* Variance\n",
    "    + Informal: Inaccurate predictions resulting from over trained model.\n",
    "    + Formal: The expected value of the squared deviation of the results from the mean of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias and variance concept](../images/bias-and-variance-concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias and variance explanation](../images/bias-and-variance-explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias and variance questions](../images/bias-and-variance-questions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias equation](../images/bias-equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Variance equation](../images/variance-equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias and variance target](../images/bias-and-variance-target.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Find a partner and draw archery results illustrating the following 4 scenarios:\n",
    "A) Low bias, low variance\n",
    "B) Medium bias, high variance\n",
    "C) High bias, low variance\n",
    "D) Low bias, high variance\n",
    "\n",
    "Who’s the best archer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Figure 18.1 ](../images/Figure-S18-1-overfitting.png)\n",
    "\n",
    "* Which of models do you like best?  Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overfitting results in poor predictive preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Apply Ocam's Razor and choose the simplest model that models the data well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### A Visual Introduction\n",
    "![Decision Trees](../images/Decision_Trees_web.png)\n",
    "\n",
    "Cite: Albon, Chris. “Machine Learning Flashcards.” Machine Learning Flashcards, 2018, machinelearningflashcards.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Interactive Demo\n",
    "\n",
    "Take a few minutes and work through these outstanding dynamic visualizations at: [www.r2d3.us/](http://www.r2d3.us/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "Pros: Computationally cheap to use, easy for humans to understand learned results, missing values OK, can deal with irrelevant features\n",
    "\n",
    "Cons: Prone to overfitting\n",
    "\n",
    "Works with: Numeric values, nominal values\n",
    "\n",
    "Cite: “3.1 Tree Construction.” Machine Learning in Action, by Peter Harrington, Manning Publications Co., 2012, pp. 39–60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Picking a Restaurant\n",
    "\n",
    "* What factors are important to you when you decide where to eat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../images/Figure-S18-3-dining-examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Which feature should we use as a label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example Dataset\n",
    "\n",
    "* How would you go about deciding where to eat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../images/Figure-S18-2-dining-decision-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Importance\n",
    "\n",
    "* What feature should we split on?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../images/Figure-S18-4-dining-best-splits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Measuring Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\large\n",
    "\\text{Entropy:} \\;\n",
    "H(V) = \\sum_{k} P(v_k) \\log_{2} \\left( \\frac{1}{P(v_k)} \\right) = - \\sum_{k} P(v_k) \\log_{2} P(v_k)\\\\\n",
    "\\begin{align}\n",
    "&V \\;\\text{random variable}\\\\\n",
    "&P \\; \\text{probability}\n",
    "\\newline\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\large\n",
    "\\text{Information Gain:} \\; \n",
    "IG(S, C) = H(S) - \\sum_{k} \\frac{\\aleph\\left({C_i}\\right)}{\\aleph\\left({S}\\right)} \\log_{2} H(C_i) \\\\\n",
    "\\begin{align}\n",
    "&S \\; \\text{parent node}\\\\\n",
    "&C_i \\; \\text{ith child node}\\\\\n",
    "&IG(S, C) \\; \\text{entropy gain from split}\\\\\n",
    "&H(S) \\; \\text{entropy of }S\\\\\n",
    "&H(C_i) \\; \\text{entropy of } C_i\\\\\n",
    "&\\aleph\\left({C_i}\\right) \\; \\text{number of elements in } C_i\\\\\n",
    "&\\aleph\\left({S}\\right) \\; \\text{number of elements in } S\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alogorithm\n",
    "\n",
    "1. Iterate over available features\n",
    "\n",
    "1. Find the with the greatest information gain with respect to the goal/target/label\n",
    "\n",
    "1. Create a node representing that feature\n",
    "\n",
    "1. Create an edge for all possible feature values\n",
    "\n",
    "1. Remove feature from further consideration\n",
    "\n",
    "1. Continue until the stopping criteria is met\n",
    "\n",
    "    * All features have been consummed\n",
    "    \n",
    "    * Each node contains examples that have the same goal/target/label value\n",
    "    \n",
    "    * Information gain is zero or smaller than a cutoff threshold value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculating the Best Split -- An Illustrated Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Calculating the best split 1](../images/decision-tree-splits-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Calculating the best split 2](../images/decision-tree-splits-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Calculating the best split 3](../images/decision-tree-splits-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Calculating the best split 4](../images/decision-tree-splits-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Calculating the best split 5](../images/decision-tree-splits-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Calculating the best split 6](../images/decision-tree-splits-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Calculating the best split 7](../images/decision-tree-splits-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Calculating the best split 8](../images/decision-tree-splits-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Break \n",
    "\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "_Note: End with humor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
